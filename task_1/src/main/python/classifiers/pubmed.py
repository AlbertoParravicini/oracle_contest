#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec  4 14:41:33 2018

@author: aparravi
"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
import random 
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.metrics import accuracy_score


def hamming_accuracy(prediction, true_values):
    """
    Metric used in multioutput-label classification,
    for each example measures the % of correctly predicted labels.
    
    Equivalent to traditional accuracy in a single-output scenario;
    """
    return np.sum(np.equal(prediction, true_values)) / float(true_values.size)

def get_score(prediction, true_values):    
    print('Hamming accuracy: {:.2f}'.format(hamming_accuracy(prediction, true_values)))
    print('Accuracy: {:.2f}'.format(accuracy_score(prediction, true_values)))

#%%

if __name__ == "__main__":
    
    graph_name = "pubmed"
    
    # Load the embeddings;
    embeddings_path = f"../../../../data/pgx-graphs/{graph_name}/embeddings.csv"
    embeddings_df = pd.read_csv(embeddings_path, header=None, index_col=0)
    embeddings_df.columns = ["e_" + str(col) for col in embeddings_df.columns]

    # Read the graph to obtain the vertex features and classes;
    vertices_path = f"../../../../data/pgx-graphs/{graph_name}/{graph_name}_v.csv"
    
    vertices_df = pd.read_csv(vertices_path, sep=",", index_col="id")
    
    
    #%% Add each vertex embedding to the vertices df;
    vertices_with_e = pd.merge(vertices_df, embeddings_df, left_index=True, right_index=True)
    
    # Remove useless columns;
    vertices_with_e.drop(columns=["summary", "tf-idf"], inplace=True)
    
    # Divide in features and labels;
    X = vertices_with_e.iloc[:, 1:].values
    y = vertices_with_e.iloc[:, 0].values
        
    seed = random.randint(0 ,2**32)
    # Create train, test and validation sets.
    # First, obtain a validation set as 20% of the data;
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)
    # Then, split again to obtain a 60% and 20% t and test (size w.r.t. original data);
    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, stratify=y_train)
    
    # Define crossvalidation.
    # Note that we are not using the test set when doing crossvalidation,
    #  as test sets are generated by crossvalidation itself.
    # Similarly to the DeepWalk paper, we can use the test data as "discarded" data to test the impact of having less labeled data.
    # If the goal is to get a good accuracy, don't discard any data (i.e. test_size=0 in the second split)!
    kfolds = StratifiedKFold(n_splits=10)
    
    
    #%% Create a classifier for the problem;
    
    model = RandomForestClassifier(random_state=seed)
    
    param_grid = {
            "n_estimators": [50, 100, 120, 150],
            "max_depth": [None],
            "max_features": [50, 90, 120],
            "min_samples_split": [1, 2, 3],
            "min_samples_leaf": [1, 2],
            "bootstrap": [False, True]
            }
 
    grid_clf = GridSearchCV(model, param_grid, cv=10, verbose=2, n_jobs=4)
    grid_clf.fit(X_train, y_train)
    
    print("\n-------- BEST ESTIMATOR --------\n")
    print(grid_clf.best_estimator_)
    print("\n-------- BEST PARAMS --------\n")
    print(grid_clf.best_params_)
    print("\n-------- BEST SCORE --------\n")
    print(grid_clf.best_score_)
    
    # Test with crossvalidation;
#    scores = cross_val_score(model, X_train, y_train, cv=kfolds, n_jobs=2, verbose=2)
#    print(f"Scores: {np.mean(scores)}")
    
    
    #%% Train on the entire training set;
    
    model.fit(X_train, y_train)
    
    # Printing train scores;
    print("Train accuracy")
    y_train_pred = model.predict(X_train)
    get_score(y_train_pred, y_train)
    
    # Printing test scores;
    print("\nValidation accuracy")
    y_val_pred = model.predict(X_val)
    get_score(y_val_pred, y_val)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    