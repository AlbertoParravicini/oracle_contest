#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec  4 14:41:33 2018

@author: aparravi
"""

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
import random 
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.metrics import f1_score

import classifier_utils as utils


#%%

if __name__ == "__main__":
    
    graph_name = "pubmed"
    
    # Load the embeddings;
    embeddings_path = f"../../../../data/pgx-graphs/{graph_name}/embeddings.csv"
    embeddings_df = pd.read_csv(embeddings_path, header=None, index_col=0)
    embeddings_df.columns = ["e_" + str(col) for col in embeddings_df.columns]

    # Read the graph to obtain the vertex features and classes;
    vertices_path = f"../../../../data/pgx-graphs/{graph_name}/{graph_name}_v.csv"
    
    vertices_df = pd.read_csv(vertices_path, sep=",", index_col="id")
    
    
    #%% Add each vertex embedding to the vertices df;
    vertices_with_e = pd.merge(vertices_df, embeddings_df, left_index=True, right_index=True)
    
    # Remove useless columns;
    vertices_with_e.drop(columns=["summary", "tf-idf"], inplace=True)
    
    # Divide in features and labels (keep up to 501 to use only graph features, keep from 501 to use only embeddings);
    X = vertices_with_e.iloc[:, 1:].values
    y = vertices_with_e.iloc[:, 0].values
        
    seed = random.randint(0 ,2**32)
    # Create train, test and validation sets.
    # First, obtain a validation set as 20% of the data;
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)
    # Then, split again to obtain a 60% and 20% t and test (size w.r.t. original data);
    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, stratify=y_train)
    
    
    #%% Create a classifier for the problem;
    
    model = RandomForestClassifier(
            random_state=seed
            )

    #%%
    # Find the best thyperparameters using grid search with crossvalidation; 
    
    param_grid = {
            "n_estimators": [50, 100, 120, 150],
            "max_depth": [None],
            "max_features": [50, 90, 120],
            "min_samples_split": [2, 3],
            "min_samples_leaf": [1, 2],
            "bootstrap": [False, True]
            }
 
    grid_clf = GridSearchCV(model, param_grid, cv=10, verbose=2, n_jobs=4, scoring="f1_micro")
    grid_clf.fit(X_train, y_train)
    
    print("\n-------- Best Estimator --------\n")
    print(grid_clf.best_estimator_)
    print("\n-------- Best Parameters --------\n")
    print(grid_clf.best_params_)
    print("\n-------- Best Score --------\n")
    print(grid_clf.best_score_)
    
    #%%    
    # As alternative, test with crossvalidation the specified model.
    # Note that we are not using the test set when doing crossvalidation,
    #  as test sets are generated by crossvalidation itself.
    # Similarly to the DeepWalk paper, we can use the test data as "discarded" data to test the impact of having less labeled data.
    # If the goal is to get a good accuracy, don't discard any data (i.e. test_size=0 in the second split)!
    kfolds = StratifiedKFold(n_splits=10)
    scores = cross_val_score(model, X_train, y_train, cv=kfolds, n_jobs=2, verbose=2, scoring="f1_micro")
    print(f"Scores: {np.mean(scores)}")
    
    
    #%% Train on the entire training set;
    
    model.fit(X_train, y_train)
    
    # Printing train scores;
    print("Train accuracy:")
    y_train_pred = model.predict(X_train)
    utils.get_score(y_train_pred, y_train)
    
    # Printing test scores;
    print("\nValidation accuracy:")
    y_val_pred = model.predict(X_val)
    utils.get_score(y_val_pred, y_val)
    
    # Validation accuracy of Random Forest, using 60% training and 20% validation.
    #   Embedding + features: 90%
    #   Embedding only: 73%
    #   Features only: 88%
    
    
    #%% Follow https://mila.quebec/wp-content/uploads/2018/07/d1ac95b60310f43bb5a0b8024522fbe08fb2a482.pdf
    
    # Use a very small training set of only 60 examples, and test on 1000 examples;
    # Repeat 10 times, and average results. Also use 500 validation examples;
    
    vertices_train, vertices_val = train_test_split(vertices_with_e, test_size=500, random_state=seed)
    X_val = vertices_val.iloc[:, 1:].values
    y_val = vertices_val.iloc[:, 0].values
    
    num_tests = 10
    scores = []
    train_scores = []
    seed = random.randint(0 ,2**32)
    for _ in range(num_tests):
        
        # Creating a balanced sample requires to operate on the original dataframe;
        vertices_t, vertices_test = train_test_split(vertices_train, test_size=1000, random_state=seed)
        X_v = vertices_val.iloc[:, 1:].values
        y_v = vertices_val.iloc[:, 0].values
        
        # Get 20 samples per class;
        X1 = vertices_train[vertices_train["label"] == 1].sample(20)
        X2 = vertices_train[vertices_train["label"] == 2].sample(20)
        X3 = vertices_train[vertices_train["label"] == 3].sample(20)
        
        vertices_concat = pd.concat([X1, X2, X3])
        
        X_t = vertices_concat.iloc[:, 1:].values
        y_t = vertices_concat.iloc[:, 0].values
        
        model.fit(X_t, y_t)
        y_val_pred = model.predict(X_v)
        train_scores += [f1_score(model.predict(X_t), y_t, average="micro")]
        scores += [f1_score(y_val_pred, y_v, average="micro")]
        
    print(f"Mean micro-f1, cross-validation, train set: {np.mean(train_scores):.3f}")
    print(f"Mean micro-f1, cross-validation: test set: {np.mean(scores):.3f}")
    
    # Predict on the validation set;
    print(f"\nValidation accuracy")
    y_val_pred = model.predict(X_val)
    utils.get_score(y_val_pred, y_val)
    
    
    # Validation accuracy of Random Forest, using 60% training and 20% validation.
    #   Embedding + features: 71%
    #   Embedding only: 59%
    #   Features only: 63%
    
    
    
    
    
    
    
    
    
    