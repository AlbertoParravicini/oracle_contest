#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec  4 14:41:33 2018

@author: aparravi
"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
import random 
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.metrics import accuracy_score


def hamming_accuracy(prediction, true_values):
    """
    Metric used in multioutput-label classification,
    for each example measures the % of correctly predicted labels.
    
    Equivalent to traditional accuracy in a single-output scenario;
    """
    return np.sum(np.equal(prediction, true_values)) / float(true_values.size)

def get_score(prediction, true_values):    
    print('Hamming accuracy: {:.2f}'.format(hamming_accuracy(prediction, true_values)))
    print('Accuracy: {:.2f}'.format(accuracy_score(prediction, true_values)))

#%%

if __name__ == "__main__":
    
    graph_name = "ppi"
    
    # Load the embeddings;
    embeddings_path = f"../../../../data/pgx-graphs/{graph_name}/embeddings.csv"
    embeddings_df = pd.read_csv(embeddings_path, header=None, index_col=0)
    embeddings_df.columns = ["e_" + str(col) for col in embeddings_df.columns]

    # Read the graph to obtain the vertex features and classes;
    vertices_path = f"../../../../data/pgx-graphs/{graph_name}/{graph_name}_v.csv"
    
    vertices_df = pd.read_csv(vertices_path, sep=",", index_col="id")
    
    # Use a temporary dict representation to turn labels and features into independent columns;
    vertices_dict = []
    for i, row_i in vertices_df.iterrows():
        labels = [int(x) for x in row_i["labels"].split(";")]
        features = [float(x) for x in row_i["features"].split(";")]
        
        new_v = {"id": vertices_df.index[i], "dataset": row_i["dataset"]}
        for j, l in enumerate(labels):
            new_v[f"label_{j}"] = l
        for j, f in enumerate(features):
            new_v[f"feature_{j}"] = f
        vertices_dict += [new_v]
    
    vertices_df = pd.DataFrame(vertices_dict)
    cols = list(vertices_df)
    
    # Move the labels at the start;
    cols = cols[52:] + cols[:52]
    cols.insert(0, cols.pop(cols.index('id')))
    cols.insert(1, cols.pop(cols.index('dataset')))
    vertices_df = vertices_df.loc[:, cols]
    vertices_df = vertices_df.set_index("id")
    
    
    #%% Add each vertex embedding to the vertices df;
    vertices_with_e = pd.merge(vertices_df, embeddings_df, left_index=True, right_index=True)
    
    # Divide the data in train, test, and validation,
    #  according to the original paper (20 train graphs, 2 test and 2 validation graphs);
    # https://arxiv.org/pdf/1706.02216.pdf
    v_train = vertices_with_e[vertices_with_e["dataset"] == "train"]
    v_test = vertices_with_e[vertices_with_e["dataset"] == "test"]
    v_val = vertices_with_e[vertices_with_e["dataset"] == "val"]
    
    # Divide in features and labels (keep up to 172 in X_train to use only graph features, keep from 172 to use only embeddings);
    X_train = v_train.iloc[:, 122:].values
    y_train = v_train.iloc[:, 1:122].values
    
    X_test = v_test.iloc[:, 122:].values
    y_test = v_test.iloc[:, 1:122].values
    
    X_val = v_val.iloc[:, 122:].values
    y_val = v_val.iloc[:, 1:122].values
    
    # Define crossvalidation.
    # Note that we are not using the test set when doing crossvalidation,
    #  as test sets are generated by crossvalidation itself.
    # We can also use the test set instead of crossvalidation, but the accuracy estimation is usually worse;
    kfolds = KFold(n_splits=10)
   
    
    #%% Create a classifier for the problem;
    
    seed = random.randint(0, 2**32)
    
    model = RandomForestClassifier(
            random_state=seed,
#            n_estimators=100,
#            max_depth=None,
#            min_samples_split=2,
#            max_features=50,
            )
    
    # Test with crossvalidation  the specified model;
    scores = cross_val_score(model, X_train, y_train, cv=kfolds, n_jobs=2, verbose=2)
    print(f"Scores: {np.mean(scores)}")
    
    # TODO: implement micro macro f1 multilabel
    
    #%% Train on the entire training set;
    
    model.fit(X_train, y_train)
    
    # Printing train scores;
    print("Train accuracy")
    y_train_pred = model.predict(X_train)
    get_score(y_train_pred, y_train)
    
    # Printing test scores;
    print("\nValidation accuracy")
    y_val_pred = model.predict(X_val)
    get_score(y_val_pred, y_val)
    
    # Validation accuracy of Random Forest, using 60% training and 20% validation.
    #   Embedding + features:
    #   Embedding only:
    #   Features only:
    
    

    
    
    
    
    
    
    
    
    